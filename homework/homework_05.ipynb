{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Week 5 Homework </center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "For this homework we will be using the FHV 2019-10 data found here. FHV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile homework questions container\n",
    "questions = {\n",
    "    1: \"What's the spark version?\",\n",
    "    2: \"What is the average size of the parquet (ending with .parquet extension) files that were created (in MB)?\",\n",
    "    3: \"How many taxi trips were there on the 15th of October? NOTE: Consider only trips that started on the 15th of October.\",\n",
    "    4: \"What is the length of the longest trip in the dataset in hours?\",\n",
    "    5: \"Spark’s User Interface which shows the application's dashboard runs on which local port?\",\n",
    "    6: \"Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location zone?\",\n",
    "}\n",
    "\n",
    "# Compile the homework solutions container\n",
    "solutions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating the data folders and downloading the data\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the raw data and the output directory\n",
    "url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz'\n",
    "file_name = 'fhv_tripdata_2019-10.csv.gz'\n",
    "input_path = '../data/raw/fhv/2019/10/' + file_name\n",
    "output_path = '../data/pq/fhv/2019/10/'\n",
    "\n",
    "# If the data is not present, download it\n",
    "if os.path.exists(input_path):\n",
    "    # Log the event\n",
    "    print('Recreating the data folders and downloading the data')\n",
    "\n",
    "    # Delete the data folders\n",
    "    shutil.rmtree(\"../data\", ignore_errors=True)\n",
    "\n",
    "    # Re-create the data folders\n",
    "    os.makedirs(input_path, exist_ok=True)\n",
    "\n",
    "    # Download the data\n",
    "    os.system(f'wget -q {url}')\n",
    "\n",
    "    # Move the data to the correct folder\n",
    "    os.system(f\"mv {file_name} {input_path}\")\n",
    "\n",
    "else:\n",
    "    # Log the event\n",
    "    print('FHV file already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Install `Spark` and `PySpark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Spark and PySpark using these [guides](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)\n",
    "\n",
    "> What is the spark version being used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/02 07:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "Spark UI running on http://localhost:4040\n",
      "\n",
      "Question 1:\tWhat's the spark version?\n",
      "Solution 1:\tspark version: 3.3.2\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('homework-05') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display spark UI\n",
    "print(f\"\\nSpark UI running on http://localhost:{spark.sparkContext.uiWebUrl.split(':')[-1]}\")\n",
    "\n",
    "# Record the current spark version\n",
    "solutions[1] = f'spark version: {spark.version}'\n",
    "\n",
    "# Display the spark version\n",
    "print()\n",
    "print(f'Question 1:\\t{questions[1]}')\n",
    "print(f'Solution 1:\\t{solutions[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "2. Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "> What is the average size of the parquet (ending with .parquet extension) files that were created (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   null|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the FHV data\n",
    "fhv_schema = T.StructType([\n",
    "    T.StructField('dispatching_base_num', T.StringType(), True),\n",
    "    T.StructField('pickup_datetime', T.TimestampType(), True),\n",
    "    T.StructField('dropoff_datetime', T.TimestampType(), True),\n",
    "    T.StructField('PULocationID', T.IntegerType(), True),\n",
    "    T.StructField('DOLocationID', T.IntegerType(), True),\n",
    "    T.StructField('SR_Flag', T.StringType(), True),\n",
    "    T.StructField('Affiliated_base_number', T.StringType(), True),\n",
    "])\n",
    "\n",
    "# Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(fhv_schema) \\\n",
    "    .csv(input_path)\n",
    "\n",
    "# Display the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 2: What is the average size of the parquet (ending with .parquet extension) files that were created (in MB)?\n",
      "Solution 2: 6.4 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "df \\\n",
    "    .repartition(6) \\\n",
    "    .write.parquet(output_path)\n",
    "\n",
    "# Get the file sizes in megabytes\n",
    "def format_filesize(size: float) -> str:\n",
    "    \"\"\"Formats a filesize in bytes to a human-readable string.\n",
    "\n",
    "    Args:\n",
    "        size: The size of the file in bytes.\n",
    "\n",
    "    Returns:\n",
    "        A human-readable string representing the size of the file.\n",
    "    \"\"\"\n",
    "    suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "    index = 0\n",
    "\n",
    "    while size >= 1024 and index < len(suffixes) - 1:\n",
    "        size /= 1024\n",
    "        index += 1\n",
    "\n",
    "    return f\"{size:.1f} {suffixes[index]}\"\n",
    "\n",
    "# Get the size of the parquet files\n",
    "parquet_files = glob.glob(output_path + '/*.parquet')\n",
    "parquet_sizes = [*map(os.path.getsize, parquet_files)]\n",
    "avg_parquet_size = round(sum(parquet_sizes) / len(parquet_files))\n",
    "\n",
    "# Record the average size of the parquet files\n",
    "solutions[2] = format_filesize(avg_parquet_size)\n",
    "\n",
    "# Display the solution\n",
    "print()\n",
    "print('Question 2:', questions[2])\n",
    "print('Solution 2:', solutions[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Count records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How many taxi trips were there on the 15th of October?\n",
    ">\n",
    "> NOTE: Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 3:\tHow many taxi trips were there on the 15th of October? NOTE: Consider only trips that started on the 15th of October.\n",
      "Solution 3:\t62,610 trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the data by the pickup date\n",
    "pickup_date = '2019-10-15'\n",
    "filtered_df = df.filter(F.col('pickup_datetime').cast('date') == pickup_date)\n",
    "record_count = filtered_df.count()\n",
    "\n",
    "# Record the number of trips on the 15th of October\n",
    "solutions[3] = '{:,} trips'.format(record_count)\n",
    "\n",
    "# Display the solution\n",
    "print()\n",
    "print(f'Question 3:\\t{questions[3]}')\n",
    "print(f'Solution 3:\\t{solutions[3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Longest trip for each day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 4:\tWhat is the length of the longest trip in the dataset in hours?\n",
      "Solution 4:\t631,152.5 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate trip duration in seconds\n",
    "df = df.withColumn(\"duration_seconds\", F.unix_timestamp(\"dropoff_datetime\") - F.unix_timestamp(\"pickup_datetime\"))\n",
    "\n",
    "# Convert duration to hours and get the trip with the longest duration\n",
    "longest_trip = df.select(\"duration_seconds\").orderBy(F.col(\"duration_seconds\").desc()).first()[0] / 3600\n",
    "\n",
    "# Optionally, join this result with the original DataFrame to get details of the longest trip\n",
    "# longest_trip_details = df.join(longest_trip, expr(\"df.duration_seconds == longest_trip.duration_seconds\"))\n",
    "\n",
    "# Record the length of the longest trip in the dataset\n",
    "solutions[4] = '{:,} hours'.format(longest_trip)\n",
    "\n",
    "# Display the solution\n",
    "print()\n",
    "print(f'Question 4:\\t{questions[4]}')\n",
    "print(f'Solution 4:\\t{solutions[4]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Spark UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Spark’s User Interface shows the application's dashboard runs on which local port?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 5:\tSpark’s User Interface which shows the application's dashboard runs on which local port?\n",
      "Solution 5:\t4040\n"
     ]
    }
   ],
   "source": [
    "# Spark UI port\n",
    "solutions[5] = spark.sparkContext.uiWebUrl.split(':')[-1]\n",
    "\n",
    "# Display the solution\n",
    "print()\n",
    "print(f'Question 5:\\t{questions[5]}')\n",
    "print(f'Solution 5:\\t{solutions[5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: Least frequent pickup location zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fhv data and [taxi zone lookup data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv) into a temp view in Spark\n",
    "\n",
    "> Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the taxi zone lookup data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 6:\tUsing the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location zone?\n",
      "Solution 6:\tJamaica Bay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Set the taxi zone lookup data arguments\n",
    "zone_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv'\n",
    "zone_filename = 'taxi_zone_lookup.csv'\n",
    "zone_filepath = '../data/' + zone_filename\n",
    "\n",
    "# If the data is not present, download it\n",
    "if not os.path.exists(zone_filepath):\n",
    "    # Log the event\n",
    "    print('Downloading the taxi zone lookup data')\n",
    "\n",
    "    # Download the data\n",
    "    os.system(f'wget -q {zone_url}')\n",
    "\n",
    "    # Move the data to the correct folder\n",
    "    os.system(f\"mv {zone_filename} {zone_filepath}\")\n",
    "\n",
    "else:\n",
    "    # Log the event\n",
    "    print('Taxi zone lookup data already exists')\n",
    "\n",
    "# Load the taxi zone lookup data\n",
    "zone_df = spark.read.option('header', 'true').csv(zone_filepath)\n",
    "\n",
    "# Create a temporary view for the zone data \n",
    "zone_df.createOrReplaceTempView('zones')\n",
    "df.createOrReplaceTempView('fhv')\n",
    "\n",
    "# Join the FHV data with the zone lookup data\n",
    "new_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        dispatching_base_num,\n",
    "        pickup_datetime,\n",
    "        dropoff_datetime,\n",
    "        pu.Zone as pickup_zone,\n",
    "        pu.Borough as pickup_borough,\n",
    "        do.Zone as dropoff_zone,\n",
    "        do.Borough as dropoff_borough,\n",
    "        SR_Flag,\n",
    "        Affiliated_base_number\n",
    "    FROM\n",
    "        fhv\n",
    "    LEFT JOIN\n",
    "        zones pu ON fhv.PULocationID = pu.LocationID\n",
    "    LEFT JOIN\n",
    "        zones do ON fhv.DOLocationID = do.LocationID\n",
    "\"\"\")\n",
    "\n",
    "# Group by the pickup zone and count the number of trips\n",
    "pickup_zone_counts = new_df.groupBy('pickup_zone').count()\n",
    "\n",
    "# Order the counts in ascending order and get the first row\n",
    "least_frequent_zone = pickup_zone_counts.orderBy(F.col('count').asc()).first()[0]\n",
    "\n",
    "# Record the name of the least frequent pickup location zone\n",
    "solutions[6] = least_frequent_zone\n",
    "\n",
    "# Display the solution\n",
    "print()\n",
    "print(f'Question 6:\\t{questions[6]}')\n",
    "print(f'Solution 6:\\t{solutions[6]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1:\tWhat's the spark version?\n",
      "Solution 1:\tspark version: 3.3.2\n",
      "\n",
      "\n",
      "Question 2:\tWhat is the average size of the parquet (ending with .parquet extension) files that were created (in MB)?\n",
      "Solution 2:\t6.4 MB\n",
      "\n",
      "\n",
      "Question 3:\tHow many taxi trips were there on the 15th of October? NOTE: Consider only trips that started on the 15th of October.\n",
      "Solution 3:\t62,610 trips\n",
      "\n",
      "\n",
      "Question 4:\tWhat is the length of the longest trip in the dataset in hours?\n",
      "Solution 4:\t631,152.5 hours\n",
      "\n",
      "\n",
      "Question 5:\tSpark’s User Interface which shows the application's dashboard runs on which local port?\n",
      "Solution 5:\t4040\n",
      "\n",
      "\n",
      "Question 6:\tUsing the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location zone?\n",
      "Solution 6:\tJamaica Bay\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in questions:\n",
    "    print()\n",
    "    print(f'Question {idx}:\\t{questions[idx]}')\n",
    "    print(f'Solution {idx}:\\t{solutions[idx]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
